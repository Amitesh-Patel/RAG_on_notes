{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available GPU memory: 4 GB\n"
     ]
    }
   ],
   "source": [
    "# Get GPU available memory\n",
    "import torch\n",
    "gpu_memory_bytes = torch.cuda.get_device_properties(0).total_memory\n",
    "gpu_memory_gb = round(gpu_memory_bytes / (2**30))\n",
    "print(f\"Available GPU memory: {gpu_memory_gb} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logged in successfully\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import HfApi, HfFolder\n",
    "import os\n",
    "def login_to_huggingface():\n",
    "    hf_token = \"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\"\n",
    "\n",
    "    if hf_token is None:\n",
    "        raise ValueError(\"Please set the HF_TOKEN environment variable\")\n",
    "    \n",
    "    HfFolder.save_token(hf_token)\n",
    "    print(\"Logged in successfully\")\n",
    "\n",
    "\n",
    "login_to_huggingface()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92b9c84918894bc58cded66edae1e227",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login  #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\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your available GPU memory is 4GB, you may not have enough memory to run a Gemma LLM locally without quantization.\n"
     ]
    }
   ],
   "source": [
    "# # Note: the following is Gemma focused, however, there are more and more LLMs of the 2B and 7B size appearing for local use.\n",
    "# if gpu_memory_gb < 5.1:\n",
    "#     print(f\"Your available GPU memory is {gpu_memory_gb}GB, you may not have enough memory to run a Gemma LLM locally without quantization.\")\n",
    "# elif gpu_memory_gb < 8.1:\n",
    "#     print(f\"GPU memory: {gpu_memory_gb} | Recommended model: Gemma 2B in 4-bit precision.\")\n",
    "#     use_quantization_config = True\n",
    "#     model_id = \"google/gemma-2b-it\"\n",
    "# elif gpu_memory_gb < 19.0:\n",
    "#     print(f\"GPU memory: {gpu_memory_gb} | Recommended model: Gemma 2B in float16 or Gemma 7B in 4-bit precision.\")\n",
    "#     use_quantization_config = False\n",
    "#     model_id = \"google/gemma-2b-it\"\n",
    "# elif gpu_memory_gb > 19.0:\n",
    "#     print(f\"GPU memory: {gpu_memory_gb} | Recommend model: Gemma 7B in 4-bit or float16 precision.\")\n",
    "#     use_quantization_config = False\n",
    "#     model_id = \"google/gemma-7b-it\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_id' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# print(f\"use_quantization_config set to: {use_quantization_config}\")\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_id set to: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mmodel_id\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model_id' is not defined"
     ]
    }
   ],
   "source": [
    "# print(f\"use_quantization_config set to: {use_quantization_config}\")\n",
    "print(f\"model_id set to: {model_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mrami\\OneDrive\\Desktop\\LLM\\RAG\\notebooks\\data\\file_1.html\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pyhtml2pdf import converter\n",
    "from tqdm import *\n",
    "\n",
    "paths = \"data/file_1.html\"\n",
    "path = os.path.abspath(paths)\n",
    "print(path)\n",
    "converter.convert(f'file:///{path}', f'notes.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "No wkhtmltopdf executable found: \"b''\"\nIf this file exists please check that this process can read it or you can pass path to it manually in method call, check README. Otherwise please install wkhtmltopdf - https://github.com/JazzCore/python-pdfkit/wiki/Installing-wkhtmltopdf",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\mrami\\anaconda3\\envs\\amazon\\Lib\\site-packages\\pdfkit\\configuration.py:35\u001b[0m, in \u001b[0;36mConfiguration.__init__\u001b[1;34m(self, wkhtmltopdf, meta_tag_prefix, environ)\u001b[0m\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwkhtmltopdf \u001b[38;5;241m=\u001b[39m lines[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[1;32m---> 35\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwkhtmltopdf\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: b''",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# path = os.path.abspath(paths)\u001b[39;00m\n\u001b[0;32m      3\u001b[0m path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:/Users/mrami/OneDrive/Desktop/LLM/RAG/data/file_1.html\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 4\u001b[0m \u001b[43mpdfkit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnotes.pdf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\mrami\\anaconda3\\envs\\amazon\\Lib\\site-packages\\pdfkit\\api.py:48\u001b[0m, in \u001b[0;36mfrom_file\u001b[1;34m(input, output_path, options, toc, cover, css, configuration, cover_first, verbose)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_file\u001b[39m(\u001b[38;5;28minput\u001b[39m, output_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, toc\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, cover\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, css\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     31\u001b[0m               configuration\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, cover_first\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m     32\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;124;03m    Convert HTML file or files to PDF document\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;124;03m    Returns: True on success\u001b[39;00m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 48\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[43mPDFKit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfile\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcover\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcover\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcss\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     49\u001b[0m \u001b[43m               \u001b[49m\u001b[43mconfiguration\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfiguration\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcover_first\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcover_first\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m r\u001b[38;5;241m.\u001b[39mto_pdf(output_path)\n",
      "File \u001b[1;32mc:\\Users\\mrami\\anaconda3\\envs\\amazon\\Lib\\site-packages\\pdfkit\\pdfkit.py:45\u001b[0m, in \u001b[0;36mPDFKit.__init__\u001b[1;34m(self, url_or_file, type_, options, toc, cover, css, configuration, cover_first, verbose)\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, url_or_file, type_, options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, toc\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, cover\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     42\u001b[0m              css\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, configuration\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, cover_first\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m     44\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msource \u001b[38;5;241m=\u001b[39m Source(url_or_file, type_)\n\u001b[1;32m---> 45\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfiguration \u001b[38;5;241m=\u001b[39m (\u001b[43mConfiguration\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m configuration \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     46\u001b[0m                           \u001b[38;5;28;01melse\u001b[39;00m configuration)\n\u001b[0;32m     47\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     48\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwkhtmltopdf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfiguration\u001b[38;5;241m.\u001b[39mwkhtmltopdf\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\mrami\\anaconda3\\envs\\amazon\\Lib\\site-packages\\pdfkit\\configuration.py:38\u001b[0m, in \u001b[0;36mConfiguration.__init__\u001b[1;34m(self, wkhtmltopdf, meta_tag_prefix, environ)\u001b[0m\n\u001b[0;32m     36\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mIOError\u001b[39;00m, \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m---> 38\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNo wkhtmltopdf executable found: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     39\u001b[0m                   \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIf this file exists please check that this process can \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     40\u001b[0m                   \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mread it or you can pass path to it manually in method call, \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     41\u001b[0m                   \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcheck README. Otherwise please install wkhtmltopdf - \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     42\u001b[0m                   \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://github.com/JazzCore/python-pdfkit/wiki/Installing-wkhtmltopdf\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwkhtmltopdf)\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menviron \u001b[38;5;241m=\u001b[39m environ\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menviron:\n",
      "\u001b[1;31mOSError\u001b[0m: No wkhtmltopdf executable found: \"b''\"\nIf this file exists please check that this process can read it or you can pass path to it manually in method call, check README. Otherwise please install wkhtmltopdf - https://github.com/JazzCore/python-pdfkit/wiki/Installing-wkhtmltopdf"
     ]
    }
   ],
   "source": [
    "import pdfkit\n",
    "# path = os.path.abspath(paths)\n",
    "path = \"C:/Users/mrami/OneDrive/Desktop/LLM/RAG/data/file_1.html\"\n",
    "pdfkit.from_file(path, 'notes.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\mrami\\\\OneDrive\\\\Desktop\\\\LLM\\\\RAG\\\\notebooks'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the absolute path of the current script (in the notebooks folder)\n",
    "script_dir = os.path.dirname(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "script_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_root = os.path.abspath(os.path.join(script_dir, '..'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_path = os.path.join(project_root, 'data_pdf', 'note_31.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\mrami\\\\OneDrive\\\\Desktop\\\\LLM\\\\RAG\\\\data_pdf\\\\note_31.pdf'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\mrami\\\\OneDrive\\\\Desktop\\\\LLM\\\\RAG'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "project_root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "script_dir = os.path.dirname(__name__)\n",
    "project_root = os.path.abspath(os.path.join(script_dir, '..'))\n",
    "pdf_path = os.path.join(project_root, 'data_pdf', 'notes_31.pdf')\n",
    "loader = PyPDFLoader(pdf_path)\n",
    "pages = loader.load_and_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = getpass.getpass('OpenAI API Key:')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'PydanticImportError' from 'pydantic.errors' (c:\\Users\\mrami\\anaconda3\\envs\\amazon\\Lib\\site-packages\\pydantic\\errors.cp311-win_amd64.pyd)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_community\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvectorstores\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FAISS\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_openai\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OpenAIEmbeddings\n\u001b[0;32m      4\u001b[0m faiss_index \u001b[38;5;241m=\u001b[39m FAISS\u001b[38;5;241m.\u001b[39mfrom_documents(pages, OpenAIEmbeddings())\n\u001b[0;32m      5\u001b[0m docs \u001b[38;5;241m=\u001b[39m faiss_index\u001b[38;5;241m.\u001b[39msimilarity_search(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat is docker?\u001b[39m\u001b[38;5;124m\"\u001b[39m, k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\mrami\\anaconda3\\envs\\amazon\\Lib\\site-packages\\langchain_openai\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_openai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchat_models\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      2\u001b[0m     AzureChatOpenAI,\n\u001b[0;32m      3\u001b[0m     ChatOpenAI,\n\u001b[0;32m      4\u001b[0m )\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_openai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01membeddings\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      6\u001b[0m     AzureOpenAIEmbeddings,\n\u001b[0;32m      7\u001b[0m     OpenAIEmbeddings,\n\u001b[0;32m      8\u001b[0m )\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_openai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllms\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AzureOpenAI, OpenAI\n",
      "File \u001b[1;32mc:\\Users\\mrami\\anaconda3\\envs\\amazon\\Lib\\site-packages\\langchain_openai\\chat_models\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_openai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchat_models\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mazure\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AzureChatOpenAI\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_openai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchat_models\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ChatOpenAI\n\u001b[0;32m      4\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChatOpenAI\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAzureChatOpenAI\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      7\u001b[0m ]\n",
      "File \u001b[1;32mc:\\Users\\mrami\\anaconda3\\envs\\amazon\\Lib\\site-packages\\langchain_openai\\chat_models\\azure.py:8\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Any, Callable, Dict, List, Optional, Union\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mopenai\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moutputs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ChatResult\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpydantic_v1\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Field, SecretStr, root_validator\n",
      "File \u001b[1;32mc:\\Users\\mrami\\anaconda3\\envs\\amazon\\Lib\\site-packages\\openai\\__init__.py:8\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_os\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping_extensions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m override\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m types\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_types\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m NOT_GIVEN, NoneType, NotGiven, Transport, ProxiesTypes\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m file_from_path\n",
      "File \u001b[1;32mc:\\Users\\mrami\\anaconda3\\envs\\amazon\\Lib\\site-packages\\openai\\types\\__init__.py:5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m__future__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m annotations\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbatch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Batch \u001b[38;5;28;01mas\u001b[39;00m Batch\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimage\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Image \u001b[38;5;28;01mas\u001b[39;00m Image\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Model \u001b[38;5;28;01mas\u001b[39;00m Model\n",
      "File \u001b[1;32mc:\\Users\\mrami\\anaconda3\\envs\\amazon\\Lib\\site-packages\\openai\\types\\batch.py:7\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m List, Optional\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping_extensions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Literal\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_models\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseModel\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbatch_error\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BatchError\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbatch_request_counts\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BatchRequestCounts\n",
      "File \u001b[1;32mc:\\Users\\mrami\\anaconda3\\envs\\amazon\\Lib\\site-packages\\openai\\_models.py:21\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping_extensions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      8\u001b[0m     Unpack,\n\u001b[0;32m      9\u001b[0m     Literal,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     17\u001b[0m     runtime_checkable,\n\u001b[0;32m     18\u001b[0m )\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpydantic\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpydantic\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgenerics\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpydantic\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfields\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FieldInfo\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_types\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     25\u001b[0m     Body,\n\u001b[0;32m     26\u001b[0m     IncEx,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     33\u001b[0m     HttpxRequestFiles,\n\u001b[0;32m     34\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\mrami\\anaconda3\\envs\\amazon\\Lib\\site-packages\\pydantic\\generics.py:4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"The `generics` module is a backport module from V1.\"\"\"\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_migration\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m getattr_migration\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;21m__getattr__\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mgetattr_migration\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\mrami\\anaconda3\\envs\\amazon\\Lib\\site-packages\\pydantic\\_migration.py:259\u001b[0m, in \u001b[0;36mgetattr_migration\u001b[1;34m(module)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Implement PEP 562 for objects that were either moved or removed on the migration\u001b[39;00m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;124;03mto V2.\u001b[39;00m\n\u001b[0;32m    251\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    256\u001b[0m \u001b[38;5;124;03m    A callable that will raise an error if the object is not found.\u001b[39;00m\n\u001b[0;32m    257\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    258\u001b[0m \u001b[38;5;66;03m# This avoids circular import with errors.py.\u001b[39;00m\n\u001b[1;32m--> 259\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01merrors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PydanticImportError\n\u001b[0;32m    261\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(name: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mobject\u001b[39m:\n\u001b[0;32m    262\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Raise an error if the object is not found, or warn if it was moved.\u001b[39;00m\n\u001b[0;32m    263\u001b[0m \n\u001b[0;32m    264\u001b[0m \u001b[38;5;124;03m    In case it was moved, it still returns the object.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    270\u001b[0m \u001b[38;5;124;03m        The object.\u001b[39;00m\n\u001b[0;32m    271\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'PydanticImportError' from 'pydantic.errors' (c:\\Users\\mrami\\anaconda3\\envs\\amazon\\Lib\\site-packages\\pydantic\\errors.cp311-win_amd64.pyd)"
     ]
    }
   ],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "faiss_index = FAISS.from_documents(pages, OpenAIEmbeddings())\n",
    "docs = faiss_index.similarity_search(\"What is docker?\", k=2)\n",
    "for doc in docs:\n",
    "    print(str(doc.metadata[\"page\"]) + \":\", doc.page_content[:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='docker \\nSimple Docket Image\\n \\nFROM python:3.11.3\\nFROM python:3.11.3\\nADD docker.py .\\nADD docker.py .\\n \\n \\nRUN pip install requests beautifulsoup4 \\nRUN pip install requests beautifulsoup4 \\nCMD [ \"python\",\"./docker.py\" ]\\nCMD [ \"python\",\"./docker.py\" ]\\ndocker build -t python-imdb .\\ndocker build -t python-imdb .\\ndocker run python-imdb\\ndocker run python-imdb\\ndocker run -t -i python-imdb #for running in interacative mode\\ndocker run -t -i python-imdb #for running in interacative mode\\ndocker run -it <your_image_name> /bin/bash  #get inside container \\\\\\ndocker run -it <your_image_name> /bin/bash  #get inside container \\\\\\n#or you can access through the desktop version and there you can edit it also\\n#or you can access through the desktop version and there you can edit it also\\ndocker system df # also info of space taken by docker\\ndocker system df # also info of space taken by docker\\nFROM python:3.11\\nFROM python:3.11\\nWORKDIR /code\\nWORKDIR /code\\nCOPY requirements.txt .\\nCOPY requirements.txt .\\nRUN pip install -r requirements.txt\\nRUN pip install -r requirements.txt\\nCOPY ./app ./app\\nCOPY ./app ./app\\nCMD [\"python\", \"./app/main.py\"]\\nCMD [\"python\", \"./app/main.py\"]\\nYes, exactly! The \\ndocker ps\\n command without any options shows only the running containers by default. Since it\\'s displaying a\\n \\ntable header without any container information underneath, it means there are currently no running containers.\\nThis doesn\\'t necessarily mean there are no containers at all; it just means there are no active containers at the moment. If you\\n \\nwant to see all containers, including those that are stopped, you can use the \\n-a\\n or \\n--all\\n flag with the \\ndocker ps\\n command:\\ndocker ps -a\\ndocker ps -a\\nThis command will show all containers, including those that are stopped or exited. If you see containers listed there, you can use\\n \\nthe \\ndocker rm\\n command as explained earlier to delete them.\\nTo delete stopped containers in Docker, you can use the \\ndocker rm\\n command followed by the container IDs or names. In your', metadata={'source': 'c:\\\\Users\\\\mrami\\\\OneDrive\\\\Desktop\\\\LLM\\\\RAG\\\\data_pdf\\\\notes_31.pdf', 'page': 0})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pages[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG/\n",
      "    convert_html_to_pdf.py\n",
      "    ReadME.md\n",
      "    assests/\n",
      "        image-1.png\n",
      "        image-2.png\n",
      "        image-3.png\n",
      "        image-4.png\n",
      "        image.png\n",
      "    data/\n",
      "        Advance Python Tutorial 9a38b9b4d2be4c3b953ded312c6d777f.html\n",
      "        AI 2519c2543fa5412d826b46a20e09f0ce.html\n",
      "        AI Supertool or superweapon 1ffe959c4bcb4b3696befe2e1c5e6d7f.html\n",
      "        Algorithm Website 3f705995c81d49bfa3b32a18174c988b.html\n",
      "        Amazon ML summer Challenge 68314f19e2e6455f93cccf7a328f9ad7.html\n",
      "        Anime List 9ac7ab158e4f4dada4380f5632c410b1.html\n",
      "        Applied c81d269c77b24743aca243aecff79627.html\n",
      "        Autonomous Agent db9809a5bbdb42a58c3e864981131baf.html\n",
      "        Backend e0745db90b3f4db080362a676b34eee2.html\n",
      "        BERT and Transformers 143ddded4bff4093bb7c391e35c848e1.html\n",
      "        BERT Questions 90804e8af2d54a4cb1a2065a6c212975.html\n",
      "        Blogs 482e9862177f4bd0b1e192f2f237f29d.html\n",
      "        Book d0448ae94e144864adaa4bea964c8255.html\n",
      "        ChicagoDev June Meetup 4afb58bb2d47499d81e4d8c7c68d743b.html\n",
      "        Closed - Omer Selem Project 3bb566ce05bf4235935272308c7c1b10.html\n",
      "        COA Assignment f92865628107458a8eac92efe9d3dcf2.html\n",
      "        Cold Emailing d037592f26c74f9d962b1fc31863dde6.html\n",
      "        College 5ac96d1055434e6bb89b65044d643edd.html\n",
      "        Competitive Programming ce5cc5c33f5c4d12be78a53cf269dc47.html\n",
      "        Computer Vision 95ee6f05149147728a823ce6ffc42583.html\n",
      "        Content 9ac227f9c596444a84bc19c3d4216802.html\n",
      "        Contour Detection 101 The Basics (Pt 1) Bleed AI f66df40340a14c109526887e39e63dc8.html\n",
      "        Courses 56b05fea5dc04223b4687dd58f942f40.html\n",
      "        CPP and Leetcode 635d959146f8438188987d0f7ab4f001.html\n",
      "        Data Analyst Roadmap 7523938f5db1471d98b4b634636066e8.html\n",
      "        Data Engineering Big project e702b9d26db14b168f90d1ebbf82ac47.html\n",
      "        Debate Skills in the office (Virtual) def0ab9e5c5440be938e0e51de1f9654.html\n",
      "        Deep Learning c09e186782d2447fba982a29e47cbc30.html\n",
      "        DevOps NY Offsite 7d426fa5954446929229d95725af4655.html\n",
      "        Diff between Dataset 0cc1c15bc3f7472d8f95396679484ad5.html\n",
      "        Diffusion and Generative Models a32f00c311ed4fa5b366518824a35735.html\n",
      "        docker 06d1d2151f844997bc9c130617640388.html\n",
      "        ECG disease 7205bcb84719479f993331f957c2e32a.html\n",
      "        Elastic Bean 7279a66c735945e4a6beb4fefa54ad8c.html\n",
      "        Eric Arnebäck cee5de6b4a70485182e4b1607c6adebb.html\n",
      "        Events Calendar 15cce27a332a4a55a20171559624446e.html\n",
      "        Explainable AI c415570121dc47499f7b8372deadef2d.html\n",
      "        file_1.html\n",
      "        Food Waste Hackathon 9f7e6444a7874b09bff2ca663f866da2.html\n",
      "        Foreign Program 83ebff4ff09140aa942f583fc415f9b9.html\n",
      "        GAN on LSUN bedroom dataset a3a10e2344404d4d89574f06be69058e.html\n",
      "        GDSC Interview b588c4ea04ff4a6a9272a7df4c9006fb.html\n",
      "        GoLang 0ab2f355b7474b33a2dd70cea5aa27ff.html\n",
      "        GPT Interview be4929e16a9b45dea1f2ef905b7dee27.html\n",
      "        GPT Store Idea cbc3b3ccb2fd4ad58367aa30a6ce888d.html\n",
      "        GPT token reduction 0990db3bd56144c2b03e0bcc926335f1.html\n",
      "        GSOC 4f68fdd065fd48ed84020472ac0c44e7.html\n",
      "        Homography - And how to calculate it by Siddharth  109462e9e67642489ba9719605269a39.html\n",
      "        How to make your localhost server accessible anywh 25d9f3aaf6b342398cffca2dec7f329a.html\n",
      "        HR Call 93b8a23e9655417dbfbb336a8928740e.html\n",
      "        Imported from notes 90600733b3e94efeb9ebeafc5d837931.html\n",
      "        Improve Neural Networks by using Complex Numbers b 2b35939eaefe49edb919a08ccf61dc64.html\n",
      "        index.html\n",
      "        INDIAN INSTITUTE OF INFORMATION TECHNLOGY , RANCHI 2ba2604490e242159219fd764732118b.html\n",
      "        Indian Stock Market Analysis 046c40ba6c2b4642a59b77930b1ec827.html\n",
      "        Interesting Blogs 5e0680eab3214a4681f9a104dfcf6931.html\n",
      "        Interview Questions 30ebf48601e541e4a9b603f8c87e38d2.html\n",
      "        Interview Questions caa1ad999ee44296bbd681edc804d8ed.html\n",
      "        Javascript 009398de856d4416be2259b7a126da4b.html\n",
      "        Kafka d3f48ffd808a4be99d259ebc75c273c5.html\n",
      "        LeetCode 0f7ddfe08533478d8edd4a88aab3a111.html\n",
      "        Linear Algebra 92192dcb080e4b7bb06eb9b952e87403.html\n",
      "        LLama f26f4b9f26234c37be4b1e91b6c4c82b.html\n",
      "        LLM a39a69eba2674f61b1e994b581944e4b.html\n",
      "        LLMs 0cdc0b26f9154a84b9a820e6d0016b03.html\n",
      "        Long Term Project 5117fb4894c74b838fc64300b8a4b876.html\n",
      "        LORA 479088d9457e4775b6044dae1bb1a54c.html\n",
      "        Losses ff1564ff1e214de3af5c099da2dcc424.html\n",
      "        Machine Learning 4a5ed938ee9540bcbc9f7c0bb1b7393c.html\n",
      "        Machine Learning Models da7abd1c7046400da4229f6e7bc2de39.html\n",
      "        Maths Magic 243ba161bd8d479abe3c398519dab368.html\n",
      "        MERN Developer 0acb90331dfa4052b19a5704b1caa172.html\n",
      "        MISS - CORS b39b5320c95440f4952a23ae6506b2f8.html\n",
      "        ML Algorithms f46a643539f546dbb4ac0fb464432765.html\n",
      "        ML OPs 1183955438bb41629d793ae595e59a0e.html\n",
      "        MMV Making automatically c2fa966164ed43cba57963dfe344f737.html\n",
      "        Motivation 82ec6b8745f64faf94e292c618008e3b.html\n",
      "        Multimodel Project 578ffacdc6bb42d88137e5cbaea54a72.html\n",
      "        My Links 9f7c881856614a02ad057eda211349fa.html\n",
      "        N-Gram model for different languages 1f58c8a6702a4953b4f78863919e402b.html\n",
      "        New Event b5c5cc69f82148e59fea8d096a352387.html\n",
      "        NLP Questions ded410fc6efe443aa77a3f894ee91eff.html\n",
      "        Notes 2edfd91a23c14459b1079e77a15c0c04.html\n",
      "        Notes 69dded9216144512a4ea5e5bafe8488a.html\n",
      "        Notes For Blogs 9c1c5b5d1ad64a95a6de4e137dc59cd0.html\n",
      "        Onnx and Quantization c794c18a1ef749bd92333f054a18017c.html\n",
      "        OOPs d04e2befbafb4a07a7cd06c32765b2af.html\n",
      "        Paper acd47a24458b45aea971e7d3b7f56a4f.html\n",
      "        Paper Learning 22080e50628c463183d6ed5786080691.html\n",
      "        Preparation DxD 8aa1421d4d6d447e8cc3356943c96716.html\n",
      "        Probabilistic Graphical Models - We Help Companies c2aea0a241f54246af55ee4dcaa264af.html\n",
      "        PROJECT IDEA FOR ML c0e73cf3b2b0495dbdbb7d7537853ff0.html\n",
      "        Projects and Courses 8f6152771d70425893ee56d0407e7124.html\n",
      "        Public Speaking for Newbies a9f478a712994d808321b571c5e3bfc6.html\n",
      "        PyTorch b47cff26c02c4767989b7af99516c7e5.html\n",
      "        PyTorch Examples — PyTorchExamples 1 11 documentat 03bd40847c264b5dbe78eeb8f0ceea94.html\n",
      "        RAG fce416e08c634a5292ad3331c0889075.html\n",
      "        References d468683c3df84cad8c74fff12bdc5aeb.html\n",
      "        Research changes 62d4faee1e814f22811b3f167be95759.html\n",
      "        Research Internship - List e54effb0228e479a9f729723d23257d6.html\n",
      "        Research Internship 4d3528d4da6e49719f93f9afe9ac7f74.html\n",
      "        Research Internship Resources 0a94af5414214ec5ae7bf2944f292b43.html\n",
      "        Research Paper 3aa5104a8f92491f93675f0480648e78.html\n",
      "        Research Sciences Internship Opportunities Microso b89906a683c342f7af9355eaed793e44.html\n",
      "        Resume Question 34ec9b2e1f5c4eab9d9a3800473e6c44.html\n",
      "        Road to getting 25 out 25 question write in JEE Ma 476429fe8c554d21b34ddef0f4791d84.html\n",
      "        Roadmap 0cd5b05ef5884ba48887bb6df9460974.html\n",
      "        Session_1_on_Hypothesis_Testing.pdf\n",
      "        Session_2_on_Hypothesis_Testing.pdf\n",
      "        Signal Processing in Python 6813ab978e9c4e38a550e1db3722e414.html\n",
      "        SIH Documentation 7c6d3d52d97b489d9a85800efa6daf97.html\n",
      "        Simon Willison’s Weblog 5960ae8d30b04cec9d3608bd2c500201.html\n",
      "        Soft Questions a8430a9af0c34fe1b506ec2d33d0509b.html\n",
      "        Sophosyntac e8b15c7960044b9d93190342e12451fd.html\n",
      "        Speech Recognition 0943434a8e6f484b8a1608eb7f8fbbaf.html\n",
      "        Statistics f86cc9ab2d704192958df2ba8cbe1237.html\n",
      "        Stock prices prediction using ML Introduction by Y 0cb1052d0d4840d8bad0674655a9eb04.html\n",
      "        system Design 235397e59e7748b4ba7c4304f0d0af1a.html\n",
      "        Teaching Material 4d69c84902fe42068cb1b181a068d86d.html\n",
      "        The Illustrated GPT-2 (Visualizing Transformer Lan ad786d56037e4f3f8c0eca47dfdb9748.html\n",
      "        Things i have to study d605b3d37e4844719e1635b9b4435d0b.html\n",
      "        Topic 2a1f312bc78b47d7938b3cf7caa2c6cd.html\n",
      "        Topics 3337d02518574742aa5a0b826c0ca801.html\n",
      "        Topological Sort 4ab299cfbea0426ea5d30f94483110f3.html\n",
      "        Transcript - Chunks - Remove unnecessary talks - S 9fd0d5a607e1439faf185b28bf632454.html\n",
      "        Transformer_from_umar_jamil.pdf\n",
      "        Universities to Apply 7354849888104c239a2e76dc0e498866.html\n",
      "        Web Socket 6744fd688bc2468da7747ca89c68ad3b.html\n",
      "        What to do when you have a imbalanced data major r 43cf748c2823421fa53a54fe2da9f0e6.html\n",
      "        Workspace f4e9ec0d1ce94a4982a880ee6442ecac.html\n",
      "        YOLO Project 9ec6066d6d49435aa9a45d895957ca0c.html\n",
      "    data_pdf/\n",
      "        notes_0.pdf\n",
      "        notes_1.pdf\n",
      "        notes_10.pdf\n",
      "        notes_100.pdf\n",
      "        notes_101.pdf\n",
      "        notes_102.pdf\n",
      "        notes_103.pdf\n",
      "        notes_104.pdf\n",
      "        notes_105.pdf\n",
      "        notes_106.pdf\n",
      "        notes_109.pdf\n",
      "        notes_11.pdf\n",
      "        notes_110.pdf\n",
      "        notes_111.pdf\n",
      "        notes_112.pdf\n",
      "        notes_113.pdf\n",
      "        notes_114.pdf\n",
      "        notes_115.pdf\n",
      "        notes_116.pdf\n",
      "        notes_117.pdf\n",
      "        notes_118.pdf\n",
      "        notes_119.pdf\n",
      "        notes_12.pdf\n",
      "        notes_120.pdf\n",
      "        notes_121.pdf\n",
      "        notes_122.pdf\n",
      "        notes_123.pdf\n",
      "        notes_124.pdf\n",
      "        notes_126.pdf\n",
      "        notes_127.pdf\n",
      "        notes_128.pdf\n",
      "        notes_129.pdf\n",
      "        notes_13.pdf\n",
      "        notes_130.pdf\n",
      "        notes_14.pdf\n",
      "        notes_15.pdf\n",
      "        notes_16.pdf\n",
      "        notes_17.pdf\n",
      "        notes_18.pdf\n",
      "        notes_19.pdf\n",
      "        notes_2.pdf\n",
      "        notes_20.pdf\n",
      "        notes_21.pdf\n",
      "        notes_22.pdf\n",
      "        notes_23.pdf\n",
      "        notes_24.pdf\n",
      "        notes_25.pdf\n",
      "        notes_26.pdf\n",
      "        notes_27.pdf\n",
      "        notes_28.pdf\n",
      "        notes_29.pdf\n",
      "        notes_3.pdf\n",
      "        notes_30.pdf\n",
      "        notes_31.pdf\n",
      "        notes_32.pdf\n",
      "        notes_33.pdf\n",
      "        notes_35.pdf\n",
      "        notes_36.pdf\n",
      "        notes_37.pdf\n",
      "        notes_38.pdf\n",
      "        notes_39.pdf\n",
      "        notes_4.pdf\n",
      "        notes_40.pdf\n",
      "        notes_41.pdf\n",
      "        notes_42.pdf\n",
      "        notes_43.pdf\n",
      "        notes_44.pdf\n",
      "        notes_45.pdf\n",
      "        notes_46.pdf\n",
      "        notes_47.pdf\n",
      "        notes_48.pdf\n",
      "        notes_49.pdf\n",
      "        notes_5.pdf\n",
      "        notes_50.pdf\n",
      "        notes_51.pdf\n",
      "        notes_52.pdf\n",
      "        notes_53.pdf\n",
      "        notes_54.pdf\n",
      "        notes_55.pdf\n",
      "        notes_56.pdf\n",
      "        notes_57.pdf\n",
      "        notes_58.pdf\n",
      "        notes_59.pdf\n",
      "        notes_6.pdf\n",
      "        notes_60.pdf\n",
      "        notes_61.pdf\n",
      "        notes_62.pdf\n",
      "        notes_63.pdf\n",
      "        notes_64.pdf\n",
      "        notes_65.pdf\n",
      "        notes_66.pdf\n",
      "        notes_67.pdf\n",
      "        notes_68.pdf\n",
      "        notes_69.pdf\n",
      "        notes_7.pdf\n",
      "        notes_70.pdf\n",
      "        notes_71.pdf\n",
      "        notes_72.pdf\n",
      "        notes_73.pdf\n",
      "        notes_74.pdf\n",
      "        notes_75.pdf\n",
      "        notes_76.pdf\n",
      "        notes_77.pdf\n",
      "        notes_78.pdf\n",
      "        notes_79.pdf\n",
      "        notes_8.pdf\n",
      "        notes_80.pdf\n",
      "        notes_81.pdf\n",
      "        notes_82.pdf\n",
      "        notes_83.pdf\n",
      "        notes_84.pdf\n",
      "        notes_85.pdf\n",
      "        notes_86.pdf\n",
      "        notes_87.pdf\n",
      "        notes_88.pdf\n",
      "        notes_89.pdf\n",
      "        notes_9.pdf\n",
      "        notes_90.pdf\n",
      "        notes_91.pdf\n",
      "        notes_92.pdf\n",
      "        notes_93.pdf\n",
      "        notes_94.pdf\n",
      "        notes_95.pdf\n",
      "        notes_96.pdf\n",
      "        notes_97.pdf\n",
      "        notes_98.pdf\n",
      "        notes_99.pdf\n",
      "    notebooks/\n",
      "        test.ipynb\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def print_directory_tree(root_dir):\n",
    "    for root, dirs, files in os.walk(root_dir):\n",
    "        level = root.replace(root_dir, '').count(os.sep)\n",
    "        indent = ' ' * 4 * (level)\n",
    "        print('{}{}/'.format(indent, os.path.basename(root)))\n",
    "        subindent = ' ' * 4 * (level + 1)\n",
    "        for f in files:\n",
    "            print('{}{}'.format(subindent, f))\n",
    "\n",
    "# Usage example\n",
    "project_root = 'c:/Users/mrami/OneDrive/Desktop/LLM/RAG'\n",
    "print_directory_tree(project_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Requires !pip install PyMuPDF, see: https://github.com/pymupdf/pymupdf\n",
    "import fitz # (pymupdf, found this is better than pypdf for our use case, note: licence is AGPL-3.0, keep that in mind if you want to use any code commercially)\n",
    "from tqdm.auto import tqdm # for progress bars, requires !pip install tqdm \n",
    "\n",
    "def text_formatter(text: str) -> str:\n",
    "    \"\"\"Performs minor formatting on text.\"\"\"\n",
    "    cleaned_text = text.replace(\"\\n\", \" \").strip() # note: this might be different for each doc (best to experiment)\n",
    "\n",
    "    # Other potential text formatting functions can go here\n",
    "    return cleaned_text\n",
    "\n",
    "# Open PDF and get lines/pages\n",
    "# Note: this only focuses on text, rather than images/figures etc\n",
    "def open_and_read_pdf(pdf_path: str) -> list[dict]:\n",
    "    \"\"\"\n",
    "    Opens a PDF file, reads its text content page by page, and collects statistics.\n",
    "\n",
    "    Parameters:\n",
    "        pdf_path (str): The file path to the PDF document to be opened and read.\n",
    "\n",
    "    Returns:\n",
    "        list[dict]: A list of dictionaries, each containing the page number\n",
    "        (adjusted), character count, word count, sentence count, token count, and the extracted text\n",
    "        for each page.\n",
    "    \"\"\"\n",
    "    doc = fitz.open(pdf_path)  # open a document\n",
    "    pages_and_texts = []\n",
    "    for page_number, page in tqdm(enumerate(doc)):  # iterate the document pages\n",
    "        text = page.get_text()  # get plain text encoded as UTF-8\n",
    "        text = text_formatter(text)\n",
    "        pages_and_texts.append({\"page_number\": page_number - 41,  # adjust page numbers since our PDF starts on page 42\n",
    "                                \"page_char_count\": len(text),\n",
    "                                \"page_word_count\": len(text.split(\" \")),\n",
    "                                \"page_sentence_count_raw\": len(text.split(\". \")),\n",
    "                                \"page_token_count\": len(text) / 4,  # 1 token = ~4 chars, see: https://help.openai.com/en/articles/4936856-what-are-tokens-and-how-to-count-them\n",
    "                                \"text\": text})\n",
    "    return pages_and_texts\n",
    "\n",
    "pages_and_texts = open_and_read_pdf(pdf_path=pdf_path)\n",
    "pages_and_texts[:2]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "amazon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
